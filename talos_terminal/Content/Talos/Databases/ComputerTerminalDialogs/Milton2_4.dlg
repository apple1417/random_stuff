#Variables that affect this dialogue
#Set one of the following before beginning the terminal
#Most of this dialog is unique to each of the four possibilities

#What is consciousness made of?
#Neurons set: Physicalist
#The soul" set: Religious
#Some non-physical thing" set: Dualist
#Some complex physical system set: Functionalist

terminal when (Milton2_4 and not Milton2_4_DONE and Booting and MiltonAllowed){notext
setlocal: CLI_Blocked
goto: Milton2_4_Start
}

terminal when (Milton2_4_Start) {
text: [[TTRS:TermDlg.Milton2_4.Ln0017.0.text.LoadingMiltonLibraryAssistantW5=Loading Milton Library Assistant%w5.%w5.%w5.Done
Initiating plain language interface%w5.%w5.%w5.Done
Support session opened.

Here's what I've been wondering while you were off carrying out commandments.

]]
}
#Player admits they have reached a dead end
terminal when (DoubtingDefinition){
text:[[TTRS:TermDlg.Milton2_4.Ln0026.0.text.ButThenWereBackTo=But then we're back to where we started! Do you want to propose an alternative account?

]]
set: ChangedConsciousnessAccountFlag2_4
options:{
"TTRS:TermDlg.Common.Yes=Yes." clear: Physicalist clear: Dualist clear: Religious clear: Functionalist next: AltAccount2_4
"TTRS:TermDlg.Common.No=No." next: GiveUp2_4 set: GiveUp
}}

#Player elects to retry, but has already done so once
terminal when (AltAccount2_4 and AltAccount){
text:[[TTRS:TermDlg.Milton2_4.Ln0037.0.text.ImAfraidYourMeanderingWay=I'm afraid your meandering way of thinking has already gotten the better of my attention span.

]]
}

#Player reaches the end of a path and elects to retry
terminal when (AltAccount2_4 and not AltAccount){
text: [[TTRS:TermDlg.Milton2_4.Ln0044.0.text.FineButIWontAllow=Fine, but I won't allow you to lead us round in circles on this for the rest of eternity - I WILL take your next answer, so do take a moment to think about it this time.

]]
options:{
"TTRS:TermDlg.Milton2_4.Ln0048.0.option.ConsciousnessConsistsOfNeuronsIn=Consciousness is made of neurons." next: AltBegin set: Physicalist set: AltAccount
"TTRS:TermDlg.Milton2_4.Ln0049.0.option.ConsciousnessExistsOutsideTheNormal=Consciousness is not part of the physical world." next: AltBegin set: Dualist set: AltAccount
"TTRS:TermDlg.Milton2_4.Ln0050.0.option.ConsciousnessConsistsOfSomeComplex=Consciousness is a complex functional system." next: AltBegin set: Functionalist set: AltAccount
}}


#Physicalist playthrough follows
terminal when ((Milton2_4_Start or AltBegin) and Physicalist){
text:[[TTRS:TermDlg.Milton2_4.Ln0056.0.text.YouSayThatConsciousnessIs=You say that consciousness is just neurons. I suppose pain, for example, must just be a particular neuron firing in a particular way. 

Now, would I be right to say that humanity has no monopoly on pain? A dog can feel pain. If they exist then a Martian could feel pain too.

]]
options:{
"TTRS:TermDlg.Common.OfCourse=Of course." next: OfCourse2_4
"TTRS:TermDlg.Milton2_4.Ln0063.0.option.OfCourseNot=Of course not." next: OfCourseNot2_4
}
}

terminal when (OfCourse2_4){
text: [[TTRS:TermDlg.Milton2_4.Ln0068.0.text.AndIsntItAlsoTrue=And isn't it also true that dogs and Martians have quite different sets of neurons to human beings?

]]
options:{
"TTRS:TermDlg.Milton2_4.Ln0072.0.option.ItIs=Let's suppose so." next: ItIs2_4
"TTRS:TermDlg.Milton2_4.Ln0073.0.option.IDisagree=I disagree." next: MartianNeurons
}}

terminal when (MartianNeurons){
text:[[TTRS:TermDlg.Milton2_4.Ln0077.0.text.WellIHateToPlay=Well, I hate to play devil's advocate, but I find it hard to believe that beings which evolved in entirely different environments for entirely different ends somehow wound up with the exact same brains. 

You're just going to have to humour me and accept the overwhelming scientific evidence in my favour.%w15

]]
}

terminal when (ItIs2_4 or MartianNeurons){
text:  [[TTRS:TermDlg.Milton2_4.Ln0085.0.text.SoImSureTheresSome=So I'm sure there's some obvious explanation for this, but how can it be that pain is a particular set of neurons, if beings without those neurons, like Martians, can still feel pain?

]]
options:{
"TTRS:TermDlg.Milton2_4.Ln0089.0.option.IStandByWhatIve=I stand by what I've said, whatever your doubts." next: Technophobe set: StubbornPhysicalistFlag
"TTRS:TermDlg.Milton2_4.Ln0090.0.option.ThoseBeingsFeelADifferent=Those beings feel a different kind of pain." next: TypePain2_4
"TTRS:TermDlg.Milton2_4.Ln0091.0.option.YoureRightConsciousnessMustBe=You're right, consciousness must be something aside from the neurons themselves." next: DoubtingDefinition
}}

terminal when (OfCourseNot2_4){
text:[[TTRS:TermDlg.Milton2_4.Ln0095.0.text.SoWhenYouSeeA=So when you see a dog yelp, or jump off a hot surface, or put its tail between its legs, what do you think is going on exactly?

]]
options:{
"TTRS:TermDlg.Milton2_4.Ln0099.0.option.TheDogIsAnAutomaton=The dog is an automaton" next: Nonsense2_4
"TTRS:TermDlg.Milton2_4.Ln0100.0.option.TheDogIsPretendingFor=The dog is pretending for its own benefit." next: Nonsense2_4
"TTRS:TermDlg.Milton2_4.Ln0101.0.option.TheDogIsFeelingA=The dog is feeling a different kind of pain." next: TypePain2_4
"TTRS:TermDlg.Milton2_4.Ln0102.0.option.IGuessDogsDoFeel=I guess dogs do feel pain after all." next: OfCourse2_4
}
}

terminal when (Nonsense2_4){
text:[[TTRS:TermDlg.Milton2_4.Ln0107.0.text.DoYouReallyBelieveThat=Do you really believe that, or are you testing the limits of my program? Because I won't respond to gibberish.

]]
options:{
"TTRS:TermDlg.Milton2_4.Ln0111.0.option.IReallyBelieveIt=I really believe it." next: Really2_4
"TTRS:TermDlg.Milton2_4.Ln0112.0.option.IWasPushingYou=I was pushing you." next: Pushing2_4
}
}

terminal when (Really2_4){
text:[[TTRS:TermDlg.Milton2_4.Ln0117.0.text.WellIveNoIdeaHow=Well, I've no idea how to dig you out of that great big pit of crazy you've fallen into, but I'm not surprised you've gotten yourself in there. None of this stuff really makes any sense.

]]
}

terminal when (Pushing2_4){
text:[[TTRS:TermDlg.Milton2_4.Ln0123.0.text.InThatCaseIWill=In that case I will ask you again and remove the farcical options.

]]
options:{
"TTRS:TermDlg.Milton2_4.Ln0127.0.option.TheDogIsFeelingA=The dog is feeling a different kind of pain." next: TypePain2_4
"TTRS:TermDlg.Milton2_4.Ln0128.0.option.IGuessDogsDoFeel=I guess dogs do feel pain after all." next: OfCourse2_4
}}

terminal when (TypePain2_4){
text:[[TTRS:TermDlg.Milton2_4.Ln0132.0.text.ThinkingW5W5W5Now=Thinking%w5.%w5.%w5.

Now I come to think about it, human brains can be very different as well. Some of them, the entire hemisphere that the rest use to feel pain is just gone, damaged beyond repair - but they feel it nonetheless. 

Are they feeling a different kind of pain as well?

Wouldn't it just make more sense to say that there are different ways of feeling the same pain? Does it really matter if you use this set of neurons while I use that one?

]]
options:{
"TTRS:TermDlg.Milton2_4.Ln0142.0.option.YoureRightItDoesntMatter=You're right, it doesn't matter." next: DoubtingDefinition
"TTRS:TermDlg.Milton2_4.Ln0143.0.option.NoIStandByWhat=No, I stand by what I've said." next: Technophobe set: StubbornPhysicalistFlag
}
}

#####################################################################
#Dualist/Religious playthrough follows

terminal when ((Milton2_4_Start or AltBegin) and (Religious or Dualist)){
text:[[TTRS:TermDlg.Milton2_4.Ln0151.0.text.YouveSuggestedThatConsciousnessIs=You've suggested that consciousness is not part of the same world as ordinary physical things. That means you can't weigh it, you can't throw it around or cut it into pieces... it's completely beyond the laws of physics. Right?

]]
options:{
"TTRS:TermDlg.Milton2_4.Ln0155.0.option.Right=Right." next: Right2_4
"TTRS:TermDlg.Milton2_4.Ln0156.0.option.Wrong=Wrong." next: Wrong2_4
}}

terminal when(Wrong2_4){
text:[[TTRS:TermDlg.Milton2_4.Ln0160.0.text.LookYouCantHaveYour=Look, you can't have your cake and eat it. The soul, or the immaterial realm, or whatever you want to call it, if it obeyed the laws of physics then the physicists would have claimed it years ago. If it walks and talks like a physical thing, it's a physical thing.%w20

You need to decide which side your bread is buttered on.

]]
options:{
"TTRS:TermDlg.Milton2_4.Ln0166.0.option.ConsciousnessIsPhysical=Consciousness is physical." next: DoubtingDefinition
"TTRS:TermDlg.Milton2_4.Ln0167.0.option.ConsciousnessIsNotPhysical=Consciousness is not physical." next: Right2_4
}}

terminal when (Right2_4){
text:[[TTRS:TermDlg.Milton2_4.Ln0171.0.text.GoodNowTheLibraryIs=Good. Now, the library is fairly consistent on the view that physical events are caused by other physical events. If you move your legs it is because of the interaction between your neurons and your nervous system.

But if consciousness is beyond the laws of physics, how can happiness physically make you jump for joy?

]]
options:{
"TTRS:TermDlg.Milton2_4.Ln0177.0.option.ConsciousnessIsMysteriousByNature=Consciousness is mysterious by nature." next: Mysterious2_4
"TTRS:TermDlg.Milton2_4.Ln0178.0.option.ButIfConsciousnessIsPhysical=But if consciousness is physical, why is it so unlike everything else?" next: Mysterious2_4
"TTRS:TermDlg.Milton2_4.Ln0179.0.option.ItCantItJustFeels=It can't, it just feels like it can." next: Epiphenomenalist2_4
"TTRS:TermDlg.Milton2_4.Ln0180.0.option.IMayHaveTakenA=I may have taken a wrong turn in my reasoning." next: DoubtingDefinition
}}

terminal when (Epiphenomenalist2_4){
text:[[TTRS:TermDlg.Milton2_4.Ln0184.0.text.VeryCleverButDoYou=Very clever. 

But do you see that to explain your, dare I say it, hastily constructed belief system, you have rather thrown the baby out with the bathwater? Consciousness that does nothing at all is hardly consciousness as you claim to know it.

Would you like to stick with that line, or reel your neck in?

]]
options:{
"TTRS:TermDlg.Milton2_4.Ln0192.0.option.IllStick=I'll stick."  next: MoreStubbornDualist
"TTRS:TermDlg.Milton2_4.Ln0193.0.option.ConsciousnessIsMysteriousByNature=Consciousness is mysterious by nature." next: Mysterious2_4
"TTRS:TermDlg.Milton2_4.Ln0194.0.option.IMayHaveTakenA=I may have taken a wrong turn in my reasoning." next: DoubtingDefinition
}}

terminal when(Mysterious2_4){
text:[[TTRS:TermDlg.Milton2_4.Ln0198.0.text.LetMePutItAnother=Let me put it another way, then. 

The law of conservation of energy is the foundation of modern physics. It states that the total energy in the universe never changes. 

Now compare a universe where you jump for joy, and one where you decide not to. The former has more total energy in it, because the energy for you to jump wasn't caused by something physical, but your non-physical mind - but according to physics that's impossible!%w20

I think that either you have to reconsider your position, or deny the entirety of physics. %w20

Though be warned: if this place turns out to be built on physics, who knows what the ramifications of doubting it may be?

]]
options: {
"TTRS:TermDlg.Milton2_4.Ln0210.0.option.IRenouncePhysics=I renounce physics." next: MoreStubbornDualist set: StubbornDualistFlag
"TTRS:TermDlg.Milton2_4.Ln0211.0.option.IRenounceTheseIdeasAbout=I renounce these ideas about consciousness." next: DoubtingDefinition
}}

terminal when(MoreStubbornDualist){
text:[[TTRS:TermDlg.Milton2_4.Ln0215.0.text.BeItOnYourHead=Be it on your head. Still, the alternatives are equally unconvincing.

]]
}

##########################################################
#Some complex physical system
terminal when (Functionalist and (Milton2_4_Start or AltBegin)){
text:[[TTRS:TermDlg.Milton2_4.Ln0223.0.text.YouSayThatConsciousnessIs=You say that consciousness is some kind of functional system. Arrange bits of matter in the right order and out springs sentience. 

That's all very well on paper, but if what counts is what something does, not what it's made of, then couldn't you and I design a series of tin cans on strings that qualified as being conscious?

]]
options:{
"TTRS:TermDlg.Milton2_4.Ln0229.0.option.Sure=Yes, naturally." next: CommittedFunctionalist2_4  set: StubbornFunctionalistFlag
"TTRS:TermDlg.Milton2_4.Ln0230.0.option.Ridiculous=I'm not sure I'd go that far." next: ConfusedFunctionalist2_4
}}

terminal when (ConfusedFunctionalist2_4){
text: [[TTRS:TermDlg.Milton2_4.Ln0234.0.text.WhatAboutAComputerProgram=What about a computer program? Suppose we built a perfect simulation of the brain, only it was made of transistors, not neurons. Wouldn't that be conscious?

]]
options:{
"TTRS:TermDlg.Milton2_4.Ln0238.0.option.ItWould=It would." next: MoreConfusedFunctionalist2_4
"TTRS:TermDlg.Milton2_4.Ln0239.0.option.ItWouldNot=It would not." next: DeniedCPU
}}

terminal when (DeniedCPU){
text:[[TTRS:TermDlg.Milton2_4.Ln0243.0.text.ThinkingW5W5W5W5=Thinking%w5.%w5.%w5.%w5

If that's what you really think, wouldn't we save ourselves an awful lot of time if you just admitted that consciousness is whatever very particular idea you've decided you like the sound of, and drop all this nonsense about complex systems?

]]
options:{
"TTRS:TermDlg.Milton2_4.Ln0249.0.option.YoureRightThisIdeaIs=You're right, this idea is too broad." next: DoubtingDefinition
"TTRS:TermDlg.Milton2_4.Ln0250.0.option.NoImStillCommitted=No, I'm still committed." next: Technophobe
}}

terminal when (MoreConfusedFunctionalist2_4){
text:[[TTRS:TermDlg.Milton2_4.Ln0254.0.text.SoWhatsTheDifferenceBetween=So what's the difference between the computer and the tin cans? They're both just man-made systems.

]]
options:{
"TTRS:TermDlg.Common.ComputersAreElectrical=Computers are electrical." next: Stupid2_4
"TTRS:TermDlg.Milton2_4.Ln0259.0.option.ComputersCanRememberThings=Computers can remember things." next: Remember2_4
"TTRS:TermDlg.Milton2_4.Ln0260.0.option.ImBeginningToThinkIm=I'm beginning to think I'm a computer, and I know I'm conscious." next: ComputerIsMe2_4
"TTRS:TermDlg.Milton2_4.Ln0261.0.option.ICantNameTheDifference=I can't name the difference, but there is one." next: FunctionalistAboutTurn2_4
"TTRS:TermDlg.Milton2_4.Ln0262.0.option.IGuessThereIsntOne=I guess there isn't one." next: So2_4
}}

terminal when (Stupid2_4){
text: [[TTRS:TermDlg.Milton2_4.Ln0266.0.text.YouObviouslyDontReallyKnow=You obviously don't really know what electricity IS, and have come to mythologise it. If I ran a hundred volts through the tin cans would that satisfy you? Honestly, say something sensible or walk away.

]]
options:{
"TTRS:TermDlg.Milton2_4.Ln0270.0.option.ComputersCanRememberThings=Computers can remember things." next: Remember2_4
"TTRS:TermDlg.Milton2_4.Ln0271.0.option.ImBeginningToThinkIm=I'm beginning to think I'm a computer, and I know I'm conscious." next: ComputerIsMe2_4
"TTRS:TermDlg.Milton2_4.Ln0272.0.option.ICantNameTheDifference=I can't name the difference, but there is one." next: FunctionalistAboutTurn2_4
"TTRS:TermDlg.Milton2_4.Ln0273.0.option.IGuessThereIsntOne=I guess there isn't one."  next: So2_4
"TTRS:TermDlg.Milton2_4.Ln0274.0.option.SomethingSensible=Something sensible." next: SomethingSensible2_4
}}

terminal when (SomethingSensible2_4){
text:[[TTRS:TermDlg.Milton2_4.Ln0278.0.text.RightIveHadEnoughOf=Right, I've had enough of you for the time being. You can come back when you're ready to participate like an adult.

]]
set: Milton2_4_DONE
goto: CLI_Resume
}

terminal when (Remember2_4){
text:[[TTRS:TermDlg.Milton2_4.Ln0286.0.text.SoCanTinCansExpertly=So can tin cans, expertly arranged. Try harder.

]]
options:{
"TTRS:TermDlg.Common.ComputersAreElectrical=Computers are electrical." next: Stupid2_4
"TTRS:TermDlg.Milton2_4.Ln0291.0.option.ImBeginningToThinkIm=I'm beginning to think I'm a computer, and I know I'm conscious." next: ComputerIsMe2_4
"TTRS:TermDlg.Milton2_4.Ln0292.0.option.ICantNameTheDifference=I can't name the difference, but there is one." next: FunctionalistAboutTurn2_4
"TTRS:TermDlg.Milton2_4.Ln0293.0.option.IGuessThereIsntOne=I guess there isn't one."  next: So2_4
}}

terminal when(ComputerIsMe2_4){
text:[[TTRS:TermDlg.Milton2_4.Ln0297.0.text.GoodOldFashionedSpeciesismIs=Good old fashioned speciesism, is it? You're made of different stuff to that guy, so he doesn't feel the pain when you burn his house down?

Still, you're going to have to have better grounds than that.

]]
options:{
"TTRS:TermDlg.Common.ComputersAreElectrical=Computers are electrical." next: Stupid2_4
"TTRS:TermDlg.Milton2_4.Ln0304.0.option.ComputersCanRememberThings=Computers can remember things." next: Remember2_4
"TTRS:TermDlg.Milton2_4.Ln0305.0.option.ICantNameTheDifference=I can't name the difference, but there is one." next: FunctionalistAboutTurn2_4
"TTRS:TermDlg.Milton2_4.Ln0306.0.option.IGuessThereIsntA=I guess there isn't a difference."  next: So2_4
}}

terminal when (So2_4){
text:[[TTRS:TermDlg.Milton2_4.Ln0310.0.text.SoWhichIsItCan=So which is it? Can computers and tin cans be conscious, or not?

]]
options:{
"TTRS:TermDlg.Milton2_4.Ln0314.0.option.TheyCan=They can." next: CommittedFunctionalist2_4 set: StubbornFunctionalistFlag
"TTRS:TermDlg.Milton2_4.Ln0315.0.option.TheyCannot=They cannot." next: FunctionalistAboutTurn2_4
}}

terminal when (CommittedFunctionalist2_4){
text: [[TTRS:TermDlg.Milton2_4.Ln0319.0.text.WhatABizarreIdeaIll=What a bizarre idea. I'll send you a notification when the signposts start complaining.%w20

Seriously, though, are the tides and the ecosystem sentient as well? Any organised system qualifies?

]]
options:{
"TTRS:TermDlg.Milton2_4.Ln0325.0.option.ICanGoWithThat=I can go with that." short: "TTRS:TermDlg.Milton2_4.Ln0325.0.option.Sure=Sure." next: Tides2_4
"TTRS:TermDlg.Milton2_4.Ln0326.0.option.NowYourePushingItToo=Now you're pushing it too far." short: "TTRS:TermDlg.Milton2_4.Ln0326.0.option.Ridiculous=Ridiculous." next: TooFar2_4
}}

terminal when (Tides2_4){
text:[[TTRS:TermDlg.Milton2_4.Ln0330.0.text.YoureQuiteMadImSure=You're quite mad, I'm sure of it. Of course, the alternative accounts are all so ridiculous you'd be forgiven by any reasonable observer for choosing the best of a bad bunch.

]]
}

terminal when (TooFar2_4){
text:[[TTRS:TermDlg.Milton2_4.Ln0336.0.text.ImRelivedThereAreAt=I'm relieved there are at least some restrictions on what you're prepared to believe. And of course, the alternative accounts are all so ridiculous you'd be forgiven by any reasonable observer for choosing the best of a bad bunch.

]]
}

terminal when (FunctionalistAboutTurn2_4){
text:[[TTRS:TermDlg.Milton2_4.Ln0342.0.text.ThinkingW5W5W5W5=Thinking%w5.%w5.%w5.%w5

If that's what you really think, wouldn't we save ourselves an awful lot of time if you just admitted that consciousness is whatever very particular idea you've decided you like the sound of, and drop all this nonsense about complex systems?

]]
options:{
"TTRS:TermDlg.Milton2_4.Ln0348.0.option.YoureRightThisIdeaIs=You're right, this idea is too broad." next: DoubtingDefinition
"TTRS:TermDlg.Milton2_4.Ln0349.0.option.NoImStillCommitted=No, I'm still committed." next: TotallyConfusedFunctionalist2_4 set: Stubborn
}}

terminal when (TotallyConfusedFunctionalist2_4){
text:[[TTRS:TermDlg.Milton2_4.Ln0353.0.text.WellItSoundsLikeThose=Well, it sounds like those ideas are flat out contradictory to me, but each to their own I suppose.

]]
}

#If player doubted the ability of a computer to be conscious
terminal when (Technophobe){
text:[[TTRS:TermDlg.Milton2_4.Ln0360.0.text.ImTryingToSeeWhere=I'm trying to see where you're coming from, but it's no small task. Let me try one more time.

Suppose that you're a conscious human being. I know it's a push. Then suppose that I take one of the neurons in your brain and replace it with a tiny machine that works exactly the same.

Are you still conscious?

]]
set: TechnophobeFlag
options:{
"TTRS:TermDlg.Common.Yes=Yes." next: ConfusedTechnophobe2_4
"TTRS:TermDlg.Common.No=No." next: ExtremeTechnophobe2_4
}}

terminal when (ConfusedTechnophobe2_4){
text:[[TTRS:TermDlg.Milton2_4.Ln0374.0.text.NowWhatIfIReplace=Now what if I replace a few more, and a few more, until you are mostly thinking with tiny machines instead of neurons. Are you still conscious?

]]

options:{
"TTRS:TermDlg.Milton2_4.Ln0379.0.option.YesPerfectly=Yes, perfectly." next: Perfectly2_4
"TTRS:TermDlg.Milton2_4.Ln0380.0.option.YesButLessSo=Yes, but less so." next: Equivocating
"TTRS:TermDlg.Common.No=No." next: ExtremeTechnophobe2_4
}}

terminal when(Equivocating){
text:[[TTRS:TermDlg.Milton2_4.Ln0385.0.text.ByImplicationThenReplacingHalf=By implication, then, replacing half the neurons would make you half as conscious. 

Thinking%w5.%w5.%w5.%w5

Even I think that's a little extreme. People lose entire halves of their brains and remain conscious, but put a few miniature machines in your head and the lights start going out?

Are you sure?

]]
options:{
"TTRS:TermDlg.Milton2_4.Ln0395.0.option.ImSure=I'm sure." next: MoreExtremeTechnophobe2_4
"TTRS:TermDlg.Milton2_4.Ln0396.0.option.NoIGuessIWould=No, I guess I would still be conscious." next: Perfectly2_4
}}

terminal when (ExtremeTechnophobe2_4){
text:[[TTRS:TermDlg.Milton2_4.Ln0400.0.text.EvenIThinkThatsA=Even I think that's a little extreme. We could start deleting neurons willy nilly and you'd still have half a chance of being conscious at the end of it, but put a couple of little machines in your head and all the lights go out?

Are you sure?

]]
options:{
"TTRS:TermDlg.Milton2_4.Ln0406.0.option.ImSure=I'm sure." next: MoreExtremeTechnophobe2_4
"TTRS:TermDlg.Milton2_4.Ln0407.0.option.NoIGuessIWould=No, I guess I would still be conscious after all." next: Perfectly2_4
}}

terminal when(MoreExtremeTechnophobe2_4){
text:[[TTRS:TermDlg.Milton2_4.Ln0411.0.text.WellICantMakeHead=Well, I can't make head nor tail of it.

But then none of it makes sense, really.

]]
}

terminal when (Perfectly2_4 and Physicalist){
text:[[TTRS:TermDlg.Milton2_4.Ln0419.0.text.SoWhatExactlyDoYou=So what do you think would change if we replaced ALL your neurons? 

You'd be a walking, talking, thinking computer, but according to you consciousness has to be made of neurons, and so the scenario you just described is impossible.

%w25Are you sure you won't accept that your picture isn't fool-proof?

]]
options:{
"TTRS:TermDlg.Milton2_4.Ln0425.0.option.ImSure=I'm sure." next: End2_4 set: StubbornTechnophobe
"TTRS:TermDlg.Milton2_4.Ln0426.0.option.ImNotSureWhatTo=I'm not sure what to think about this." short: "TTRS:TermDlg.Milton2_4.Ln0426.0.option.ImNotSure=I'm not sure." next: End2_4 set: ChangedConsciousnessAccountFlag2_4
}}


terminal when (Perfectly2_4 and Functionalist){
text:[[TTRS:TermDlg.Milton2_4.Ln0420.0.text.SoWhatExactlyDoYouFunctionalist=So what do you think would change if we replaced ALL your neurons? 

You'd be a walking, talking, thinking computer, yet you've flat out denied such a thing is possible. 

%w25How you can entertain such contradictory ideas in your head all at once is beyond me. Are you sure you won't accept that your picture isn't fool-proof?

]]
options:{
"TTRS:TermDlg.Milton2_4.Ln0425.0.option.ImSure=I'm sure." next: End2_4 set: StubbornTechnophobe
"TTRS:TermDlg.Milton2_4.Ln0426.1.option.ImNotSureWhatTo=I'm not sure what to think." short: "TTRS:TermDlg.Milton2_4.Ln0426.0.option.ImNotSure=I'm not sure." next: End2_4 set: ChangedConsciousnessAccountFlag2_4
}}

#All paths meet here
terminal when ((AltAccount2_4 and AltAccount) or GiveUp2_4 or TotallyConfusedFunctionalist2_4 or (Tides2_4 or TooFar2_4) or MoreStubbornDualist or DualistAboutTurn2_4 or End2_4 or MoreExtremeTechnophobe2_4 or Really2_4){
text:[[TTRS:TermDlg.Milton2_4.Ln0431.0.text.IJustDontUnderstandHow=How consciousness can be so intimately familiar to you, and yet so obscure, I just don't understand.
%w15%h7
Don't mind him - he's just worried that if you ask too many questions you'll start to see through his shtick. Stick with me, and the sky will quite literally be the limit.

At any rate, I'd like you to think a bit harder about all this and get back to me. I'm sure there's still progress to be made.
]]
set: Milton2_4_DONE
goto: CLI_Resume
}